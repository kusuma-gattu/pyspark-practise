{"cells": [{"cell_type": "markdown", "id": "639fad21-bb33-496b-abdf-270ec315dcd3", "metadata": {"tags": []}, "source": "# Spark Session"}, {"cell_type": "code", "execution_count": 4, "id": "92ef205c-628b-4b53-a2c8-930558dad46c", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Product: string (nullable = true)\n |-- ID: integer (nullable = true)\n |-- Date: date (nullable = true)\n |-- Timestamp: timestamp (nullable = true)\n |-- Price: float (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----+----------+-------------------+-----+\n|  Product|  ID|      Date|          Timestamp|Price|\n+---------+----+----------+-------------------+-----+\n|Product A|1001|2023-07-20|2023-07-20 10:15:30|29.99|\n|Product B|1002|2023-07-19|2023-07-19 14:20:45|49.99|\n|Product C|1003|2023-07-18|2023-07-18 09:30:15|39.99|\n|Product D|1004|2023-07-17|2023-07-17 16:45:00|19.99|\n+---------+----+----------+-------------------+-----+\n\n"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom datetime import datetime\n\n# Create spark session\nspark = SparkSession.builder \\\n        .appName(\"Spark with Hive\") \\\n        .enableHiveSupport() \\\n        .getOrCreate()\n\n# Hardcoded data\ndata = [\n    [\"Product A\", 1001, datetime.strptime(\"2023-07-20\", \"%Y-%m-%d\"), datetime.strptime(\"2023-07-20 10:15:30\", \"%Y-%m-%d %H:%M:%S\"), 29.99],\n    [\"Product B\", 1002, datetime.strptime(\"2023-07-19\", \"%Y-%m-%d\"), datetime.strptime(\"2023-07-19 14:20:45\", \"%Y-%m-%d %H:%M:%S\"), 49.99],\n    [\"Product C\", 1003, datetime.strptime(\"2023-07-18\", \"%Y-%m-%d\"), datetime.strptime(\"2023-07-18 09:30:15\", \"%Y-%m-%d %H:%M:%S\"), 39.99],\n    [\"Product D\", 1004, datetime.strptime(\"2023-07-17\", \"%Y-%m-%d\"), datetime.strptime(\"2023-07-17 16:45:00\", \"%Y-%m-%d %H:%M:%S\"), 19.99]\n]\n\n# Define schema\nschema = StructType([\n    StructField(\"Product\", StringType(), True),\n    StructField(\"ID\", IntegerType(), True),\n    StructField(\"Date\", DateType(), True),\n    StructField(\"Timestamp\", TimestampType(), True),\n    StructField(\"Price\", FloatType(), True)\n])\n\n# Create dataframe\ndf = spark.createDataFrame(data, schema)\n\n# Print schema\ndf.printSchema()\n\n# Print Data\ndf.show()"}, {"cell_type": "markdown", "id": "e9db5fd9-acb8-4036-b939-8d80554a6d20", "metadata": {}, "source": "## Read Data From HDFS"}, {"cell_type": "code", "execution_count": 5, "id": "5cfa2bc6-a674-4829-9956-0ec841566a34", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- order_id: string (nullable = true)\n |-- order_item_id: integer (nullable = true)\n |-- product_id: string (nullable = true)\n |-- seller_id: string (nullable = true)\n |-- shipping_limit_date: timestamp (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 2:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|\n|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|\n|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|\n|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|\n|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# First read example should not infer schema, ignore header row, provide explicit column name and datatype\n\n# Define schema\nschema = StructType([\n    StructField(\"order_id\", StringType(), True),\n    StructField(\"order_item_id\", IntegerType(), True),\n    StructField(\"product_id\", StringType(), True),\n    StructField(\"seller_id\", StringType(), True),\n    StructField(\"shipping_limit_date\", TimestampType(), True),\n    StructField(\"price\", DoubleType(), True),\n    StructField(\"freight_value\", DoubleType(), True)\n])\n\nhdfs_path = '/spark_input_data/order_items_dataset.csv'\n\ndf = spark.read.format('csv').option('header', 'true').option('inferSchema', 'false').schema(schema).load(hdfs_path)\n\ndf.printSchema()\n\ndf.show(5)"}, {"cell_type": "markdown", "id": "6d7cce9e-d4df-4096-9c72-626ab4beb65c", "metadata": {}, "source": "## Schema Inference"}, {"cell_type": "code", "execution_count": 6, "id": "5962da0b-c0a2-46b9-b20f-c7f7b9225978", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- order_id: string (nullable = true)\n |-- order_item_id: integer (nullable = true)\n |-- product_id: string (nullable = true)\n |-- seller_id: string (nullable = true)\n |-- shipping_limit_date: timestamp (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|\n|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|\n|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|\n|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|\n|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\nonly showing top 5 rows\n\n"}], "source": "# Second read example should infer schema, ignore header row\n\nhdfs_path = '/spark_input_data/order_items_dataset.csv'\n\ndf2 = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(hdfs_path)\n\ndf2.printSchema()\n\ndf2.show(5)"}, {"cell_type": "markdown", "id": "37ff7174-3f75-4a74-8c73-4f241247a314", "metadata": {"tags": []}, "source": "## Repartition"}, {"cell_type": "code", "execution_count": 8, "id": "17861ca5-772a-47e2-b791-bf72806a131a", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Number of partitions: 2\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 6:=============================>                             (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Number of partitions: 10\n"}], "source": "# Number of partitions after reading from hdfs\n\nprint(f\"Number of partitions: {df2.rdd.getNumPartitions()}\")\n\ndf3 = df2.repartition(10)\n\n# Number of partitions after repartition\n\nprint(f\"Number of partitions: {df3.rdd.getNumPartitions()}\")\n"}, {"cell_type": "markdown", "id": "99d4209f-f020-4fc0-9876-365d9e3e1c68", "metadata": {}, "source": "## DataFrame Operations"}, {"cell_type": "code", "execution_count": 13, "id": "cdf2c494-3b76-4b81-9ef3-c24a230c4d4b", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+\n|            order_id|\n+--------------------+\n|6299bb8e855289b41...|\n|71fbb9971d84bf97a...|\n|74322a01b770c2ea3...|\n|a23fc2b3af4f1a48e...|\n|747af114bbea56ac1...|\n+--------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+\n|            order_id|shipping_limit_date|\n+--------------------+-------------------+\n|3bbf8f927f288e4a1...|2017-11-09 14:25:38|\n|50c40cfcbb6ce3fca...|2018-06-14 09:52:04|\n|51c3d73e0e9052253...|2018-02-22 19:15:27|\n|183ee0e3ebd4c1c99...|2018-02-07 20:14:08|\n|3a1400b5d4dd3082a...|2018-03-27 17:28:20|\n+--------------------+-------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 22:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+\n|                 oid|         limit_date|\n+--------------------+-------------------+\n|3bbf8f927f288e4a1...|2017-11-09 14:25:38|\n|50c40cfcbb6ce3fca...|2018-06-14 09:52:04|\n|51c3d73e0e9052253...|2018-02-22 19:15:27|\n|183ee0e3ebd4c1c99...|2018-02-07 20:14:08|\n|3a1400b5d4dd3082a...|2018-03-27 17:28:20|\n+--------------------+-------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# select columns in different options\nfrom pyspark.sql.functions import *\n\n# select one column\ndf3.select('order_id').show(5)\n# select multiple columns\ndf3.select('order_id', 'shipping_limit_date').show(5)\n# column aliasing\ndf3.select(col('order_id').alias('oid'), col('shipping_limit_date').alias('limit_date')).show(5)"}, {"cell_type": "markdown", "id": "94325582-9f26-452e-a73c-720177303bc6", "metadata": {}, "source": "## Derive New Columns using withColumn "}, {"cell_type": "code", "execution_count": 14, "id": "eaa92c94-627a-4791-9595-b4ca45ed286d", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 25:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+----+-----+\n|            order_id|shipping_limit_date|year|month|\n+--------------------+-------------------+----+-----+\n|3bbf8f927f288e4a1...|2017-11-09 14:25:38|2017|   11|\n|50c40cfcbb6ce3fca...|2018-06-14 09:52:04|2018|    6|\n|51c3d73e0e9052253...|2018-02-22 19:15:27|2018|    2|\n|183ee0e3ebd4c1c99...|2018-02-07 20:14:08|2018|    2|\n|3a1400b5d4dd3082a...|2018-03-27 17:28:20|2018|    3|\n+--------------------+-------------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df4 = df3.withColumn('year', year(col('shipping_limit_date'))).withColumn('month', month(col('shipping_limit_date')))\n\ndf4.select('order_id', 'shipping_limit_date', 'year', 'month').show(5)"}, {"cell_type": "markdown", "id": "18bfade6-35be-4182-b2a1-d917d1125a8c", "metadata": {}, "source": "## Renaming the Column"}, {"cell_type": "code", "execution_count": null, "id": "1a76f446-70c9-480e-a2fb-a62712db9f2e", "metadata": {}, "outputs": [], "source": "# renaming existing column using withColumnRenamed\n\ndf5 = df4.withColumnRenamed(\"shipping_limit_date\", \"shipping_lim"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}