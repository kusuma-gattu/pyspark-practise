{"cells": [{"cell_type": "markdown", "id": "639fad21-bb33-496b-abdf-270ec315dcd3", "metadata": {"tags": []}, "source": "# Spark Session"}, {"cell_type": "code", "execution_count": 1, "id": "92ef205c-628b-4b53-a2c8-930558dad46c", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/07/21 10:44:26 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- Product: string (nullable = true)\n |-- ID: integer (nullable = true)\n |-- Date: date (nullable = true)\n |-- Timestamp: timestamp (nullable = true)\n |-- Price: float (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----+----------+-------------------+-----+\n|  Product|  ID|      Date|          Timestamp|Price|\n+---------+----+----------+-------------------+-----+\n|Product A|1001|2023-07-20|2023-07-20 10:15:30|29.99|\n|Product B|1002|2023-07-19|2023-07-19 14:20:45|49.99|\n|Product C|1003|2023-07-18|2023-07-18 09:30:15|39.99|\n|Product D|1004|2023-07-17|2023-07-17 16:45:00|19.99|\n+---------+----+----------+-------------------+-----+\n\n"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom datetime import datetime\n\n# Create spark session\nspark = SparkSession.builder \\\n        .appName(\"Spark with Hive\") \\\n        .enableHiveSupport() \\\n        .getOrCreate()\n\n# Hardcoded data\ndata = [\n    [\"Product A\", 1001, datetime.strptime(\"2023-07-20\", \"%Y-%m-%d\"), datetime.strptime(\"2023-07-20 10:15:30\", \"%Y-%m-%d %H:%M:%S\"), 29.99],\n    [\"Product B\", 1002, datetime.strptime(\"2023-07-19\", \"%Y-%m-%d\"), datetime.strptime(\"2023-07-19 14:20:45\", \"%Y-%m-%d %H:%M:%S\"), 49.99],\n    [\"Product C\", 1003, datetime.strptime(\"2023-07-18\", \"%Y-%m-%d\"), datetime.strptime(\"2023-07-18 09:30:15\", \"%Y-%m-%d %H:%M:%S\"), 39.99],\n    [\"Product D\", 1004, datetime.strptime(\"2023-07-17\", \"%Y-%m-%d\"), datetime.strptime(\"2023-07-17 16:45:00\", \"%Y-%m-%d %H:%M:%S\"), 19.99]\n]\n\n# Define schema\nschema = StructType([\n    StructField(\"Product\", StringType(), True),\n    StructField(\"ID\", IntegerType(), True),\n    StructField(\"Date\", DateType(), True),\n    StructField(\"Timestamp\", TimestampType(), True),\n    StructField(\"Price\", FloatType(), True)\n])\n\n# Create dataframe\ndf = spark.createDataFrame(data, schema)\n\n# Print schema\ndf.printSchema()\n\n# Print Data\ndf.show()"}, {"cell_type": "markdown", "id": "e9db5fd9-acb8-4036-b939-8d80554a6d20", "metadata": {}, "source": "## Read Data From HDFS"}, {"cell_type": "code", "execution_count": 2, "id": "5cfa2bc6-a674-4829-9956-0ec841566a34", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- order_id: string (nullable = true)\n |-- order_item_id: integer (nullable = true)\n |-- product_id: string (nullable = true)\n |-- seller_id: string (nullable = true)\n |-- shipping_limit_date: timestamp (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 2:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|\n|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|\n|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|\n|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|\n|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# First read example should not infer schema, ignore header row, provide explicit column name and datatype\n\n# Define schema\nschema = StructType([\n    StructField(\"order_id\", StringType(), True),\n    StructField(\"order_item_id\", IntegerType(), True),\n    StructField(\"product_id\", StringType(), True),\n    StructField(\"seller_id\", StringType(), True),\n    StructField(\"shipping_limit_date\", TimestampType(), True),\n    StructField(\"price\", DoubleType(), True),\n    StructField(\"freight_value\", DoubleType(), True)\n])\n\nhdfs_path = '/spark_input_data/order_items_dataset.csv'\n\ndf = spark.read.format('csv').option('header', 'true').option('inferSchema', 'false').schema(schema).load(hdfs_path)\n\ndf.printSchema()\n\ndf.show(5)"}, {"cell_type": "markdown", "id": "6d7cce9e-d4df-4096-9c72-626ab4beb65c", "metadata": {}, "source": "## Schema Inference"}, {"cell_type": "code", "execution_count": 3, "id": "5962da0b-c0a2-46b9-b20f-c7f7b9225978", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- order_id: string (nullable = true)\n |-- order_item_id: integer (nullable = true)\n |-- product_id: string (nullable = true)\n |-- seller_id: string (nullable = true)\n |-- shipping_limit_date: timestamp (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|\n|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|\n|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|\n|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|\n|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\nonly showing top 5 rows\n\n"}], "source": "# Second read example should infer schema, ignore header row\n\nhdfs_path = '/spark_input_data/order_items_dataset.csv'\n\ndf2 = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(hdfs_path)\n\ndf2.printSchema()\n\ndf2.show(5)"}, {"cell_type": "markdown", "id": "37ff7174-3f75-4a74-8c73-4f241247a314", "metadata": {"tags": []}, "source": "## Repartition"}, {"cell_type": "code", "execution_count": 4, "id": "17861ca5-772a-47e2-b791-bf72806a131a", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Number of partitions: 2\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 6:=============================>                             (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Number of partitions: 10\n"}], "source": "# Number of partitions after reading from hdfs\n\nprint(f\"Number of partitions: {df2.rdd.getNumPartitions()}\")\n\ndf3 = df2.repartition(10)\n\n# Number of partitions after repartition\n\nprint(f\"Number of partitions: {df3.rdd.getNumPartitions()}\")\n"}, {"cell_type": "markdown", "id": "99d4209f-f020-4fc0-9876-365d9e3e1c68", "metadata": {}, "source": "## DataFrame Operations"}, {"cell_type": "code", "execution_count": 5, "id": "cdf2c494-3b76-4b81-9ef3-c24a230c4d4b", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+\n|            order_id|\n+--------------------+\n|6299bb8e855289b41...|\n|71fbb9971d84bf97a...|\n|74322a01b770c2ea3...|\n|a23fc2b3af4f1a48e...|\n|747af114bbea56ac1...|\n+--------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+\n|            order_id|shipping_limit_date|\n+--------------------+-------------------+\n|f0b47dadd5f372c41...|2018-05-14 04:54:53|\n|bbd319ae8e4b46101...|2017-11-24 02:28:04|\n|e16fb24453a306d5d...|2018-08-10 03:24:54|\n|d4de6d0debe2df72c...|2017-03-13 03:35:12|\n|bdbe8da70dcc6e6a2...|2018-04-22 21:52:25|\n+--------------------+-------------------+\nonly showing top 5 rows\n\n+--------------------+-------------------+\n|                 oid|         limit_date|\n+--------------------+-------------------+\n|3bbf8f927f288e4a1...|2017-11-09 14:25:38|\n|50c40cfcbb6ce3fca...|2018-06-14 09:52:04|\n|51c3d73e0e9052253...|2018-02-22 19:15:27|\n|183ee0e3ebd4c1c99...|2018-02-07 20:14:08|\n|3a1400b5d4dd3082a...|2018-03-27 17:28:20|\n+--------------------+-------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# select columns in different options\nfrom pyspark.sql.functions import *\n\n# select one column\ndf3.select('order_id').show(5)\n# select multiple columns\ndf3.select('order_id', 'shipping_limit_date').show(5)\n# column aliasing\ndf3.select(col('order_id').alias('oid'), col('shipping_limit_date').alias('limit_date')).show(5)"}, {"cell_type": "markdown", "id": "94325582-9f26-452e-a73c-720177303bc6", "metadata": {}, "source": "## Derive New Columns using withColumn "}, {"cell_type": "code", "execution_count": 6, "id": "eaa92c94-627a-4791-9595-b4ca45ed286d", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+----+-----+\n|            order_id|shipping_limit_date|year|month|\n+--------------------+-------------------+----+-----+\n|f0b47dadd5f372c41...|2018-05-14 04:54:53|2018|    5|\n|bbd319ae8e4b46101...|2017-11-24 02:28:04|2017|   11|\n|e16fb24453a306d5d...|2018-08-10 03:24:54|2018|    8|\n|d4de6d0debe2df72c...|2017-03-13 03:35:12|2017|    3|\n|bdbe8da70dcc6e6a2...|2018-04-22 21:52:25|2018|    4|\n+--------------------+-------------------+----+-----+\nonly showing top 5 rows\n\n"}], "source": "df4 = df3.withColumn('year', year(col('shipping_limit_date'))).withColumn('month', month(col('shipping_limit_date')))\n\ndf4.select('order_id', 'shipping_limit_date', 'year', 'month').show(5)"}, {"cell_type": "markdown", "id": "18bfade6-35be-4182-b2a1-d917d1125a8c", "metadata": {}, "source": "## Renaming the Column"}, {"cell_type": "code", "execution_count": 7, "id": "1a76f446-70c9-480e-a2fb-a62712db9f2e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-----------------------+\n|            order_id|shipping_limit_datetime|\n+--------------------+-----------------------+\n|f0b47dadd5f372c41...|    2018-05-14 04:54:53|\n|bbd319ae8e4b46101...|    2017-11-24 02:28:04|\n|e16fb24453a306d5d...|    2018-08-10 03:24:54|\n|d4de6d0debe2df72c...|    2017-03-13 03:35:12|\n|bdbe8da70dcc6e6a2...|    2018-04-22 21:52:25|\n+--------------------+-----------------------+\nonly showing top 5 rows\n\n"}], "source": "# renaming existing column using withColumnRenamed\n\ndf5 = df4.withColumnRenamed(\"shipping_limit_date\", \"shipping_limit_datetime\")\ndf5.select(\"order_id\", \"shipping_limit_datetime\").show(5)"}, {"cell_type": "markdown", "id": "a079b213", "metadata": {}, "source": "### Filter Conditions\n\nfilter conditions can be written 2 ways:\n1. object-based\n2. sql type of expressions"}, {"cell_type": "code", "execution_count": 8, "id": "fb4faedf", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|    2017-09-19 09:45:35| 58.9|        13.29|2017|    9|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|    2017-09-19 09:45:35| 58.9|        13.29|2017|    9|\n|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|    2017-05-03 11:05:13|239.9|        19.93|2017|    5|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|e41e7842fa15d821c...|            1|d6897f1dfdc99bbbd...|508808d438fe2ff97...|    2018-08-29 22:45:14|41.99|         8.44|2018|    8|\n|ea452bd7da485c937...|            3|ec2d43cc59763ec91...|1c129092bf23f28a5...|    2018-02-21 10:35:32| 45.9|         9.34|2018|    2|\n|f0b47dadd5f372c41...|            1|a9e0a3a894f4bf784...|8b321bb669392f516...|    2018-05-14 04:54:53| 15.9|         7.39|2018|    5|\n|a75a520af41db90aa...|            1|89321f94e35fc6d79...|16090f2ca825584b5...|    2018-04-30 04:31:08| 27.9|         7.87|2018|    4|\n|a3270f2613a807c71...|            1|c7c44bf04797742e7...|4e17c65a516f69d02...|    2018-07-19 17:31:32| 32.9|         7.49|2018|    7|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\nonly showing top 5 rows\n\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|363524b17966c3a64...|            2|43ee88561093499d9...|23613d49c3ac2bd30...|    2018-05-24 22:35:14| 10.9|          3.8|2018|    5|\n|1d9609dad08db33f3...|            1|7cc67695a7648efc5...|95e03ca3d4146e401...|    2017-12-11 18:10:31|29.99|          8.9|2017|   12|\n|50aff4b82439e01c5...|            1|ec1faa2edc27ce323...|cc419e0650a3c5ba7...|    2017-11-23 21:53:21|29.99|         7.78|2017|   11|\n|37ee401157a3a0b28...|            9|d34c07a2d817ac73f...|e7d5b006eb624f130...|    2018-04-19 02:30:52|29.99|         7.39|2018|    4|\n|8f5fac100b291e3c7...|            1|0e996644bf2835621...|b4ffb71f0cb1b1c3d...|    2017-12-08 09:13:27| 6.84|         7.78|2017|   12|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\nonly showing top 5 rows\n\n"}], "source": "# filter condition\n\ndf5.filter(col(\"order_id\") == \"00010242fe8c5a6d1ba2dd792cb16214\").show()\n\norder_id_list = [\"00010242fe8c5a6d1ba2dd792cb16214\", \"00018f77f2f0320c557190d7a144bdd3\"]\n\ndf5.filter(col(\"order_id\").isin(order_id_list)).show()\n\ndf5.filter((col(\"price\")<50) & (col(\"freight_value\")<10)).show(5)\n\n# SQL type expression\n\ndf5.filter(\"price < 50 and freight_value < 10\").show(5)"}, {"cell_type": "markdown", "id": "fee25d87", "metadata": {}, "source": "#### Drop Columns, Drop duplicates "}, {"cell_type": "code", "execution_count": 9, "id": "309054c6", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+\n|1e1bb536916a99649...|            2|0288f8dd74b931b4e...|1da3aeb70d7989d1e...|    2017-09-05 12:10:11| 49.99|        21.15|2017|\n|62a0e822dd605871a...|            1|31dbb0d1815bdc83c...|6da1992f915d77be9...|    2017-06-08 11:50:18|  29.0|        15.79|2017|\n|025c72e88fbf2358b...|            2|bef21943bc2335188...|e49c26c3edfa46d22...|    2017-03-21 21:24:27|  19.9|         20.8|2017|\n|23d16dddab46fd3d0...|            1|cca8e09ba6f2d35e4...|43f8c9950d11ecd03...|    2018-01-31 22:17:51|109.99|        14.52|2018|\n|71c0d1686c9b55563...|            2|eb6c2ecde53034fc9...|1025f0e2d44d7041d...|    2017-12-01 19:31:45| 32.99|        16.11|2017|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+\nonly showing top 5 rows\n\n"}], "source": "df5.drop('month').show(5)"}, {"cell_type": "code", "execution_count": 10, "id": "1c7fa3b0", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 42:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|    2017-05-03 11:05:13| 239.9|        19.93|2017|    5|\n|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|    2018-01-18 14:48:30| 199.0|        17.87|2018|    1|\n|00048cc3ae777c65d...|            1|ef92defde845ab845...|6426d21aca402a131...|    2017-05-23 03:55:27|  21.9|        12.69|2017|    5|\n|0005a1a1728c9d785...|            1|310ae3c140ff94b03...|a416b6a846a117243...|    2018-03-26 18:31:29|145.95|        11.65|2018|    3|\n|0005f50442cb953dc...|            1|4535b0e1091c278df...|ba143b05f0110f0dc...|    2018-07-06 14:10:56| 53.99|         11.4|2018|    7|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# drop duplicates rows based on multiple columns \ndf5.dropDuplicates([\"order_id\", \"order_item_id\"]).show(5)\n"}, {"cell_type": "markdown", "id": "fbd24970", "metadata": {}, "source": "when drop duplicates whether shuffling will happen?\n    yes, because to search data in all partitions. \n    even for distinct operation also shuffling will happen"}, {"cell_type": "code", "execution_count": 11, "id": "112ca8a8", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|f4b56fe7668e96ac9...|            1|0e1cd2215a878ad18...|17eea220a40cc0d2c...|    2017-10-05 20:14:08|158.0|        36.43|2017|   10|\n|de1f63780a06d4800...|            1|86ef699f48c083648...|850913d59ce317156...|    2018-03-15 10:15:30| 49.0|        18.23|2018|    3|\n|f93af565de05d2427...|            1|2b4609f8948be1887...|cc419e0650a3c5ba7...|    2018-06-11 12:10:28|79.99|         8.32|2018|    6|\n|dffe39ab35d34d2dc...|            1|8983a3b149303c013...|9f505651f4a6abe90...|    2018-08-22 14:30:02| 99.0|        15.79|2018|    8|\n|f2db192253fbe0e93...|            1|2701fc4808fcb783d...|7f02656561b680def...|    2017-09-29 11:27:36| 42.0|        15.98|2017|    9|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 51:=============================================>           (8 + 2) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|bb5cce57f8d80c481...|            1|6ec26b3516fecd18c...|c70c1b0d8ca86052f...|    2018-06-29 11:57:21|110.32|         8.03|2018|    6|\n|d8fd9830792304098...|            1|a00d11a2119bd70d6...|79ebd9a61bac3eaf8...|    2018-07-16 11:11:49|  99.9|        19.89|2018|    7|\n|c5119bb429cf05b92...|            1|192b332c511e484ea...|41b86b552e54e3a70...|    2017-04-04 02:15:19|  74.5|        14.69|2017|    4|\n|e1041ca455e08b097...|            1|5a6e53c3b4e8684b1...|7299e27ed73d2ad98...|    2017-03-15 11:09:46| 12.99|        14.52|2017|    3|\n|d7f4e2f755cf2f40a...|            1|cec09725da5ed0147...|4d6d651bd7684af3f...|    2017-11-30 13:14:18|  69.9|        20.98|2017|   11|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "\ndf5.distinct().show(5)\n\n# this will drop the records when entire record is duplicated. i.e. across all columns\ndf5.dropDuplicates().show(5)\n"}, {"cell_type": "markdown", "id": "f0815362", "metadata": {}, "source": "### sorting "}, {"cell_type": "code", "execution_count": 12, "id": "3892759c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|0812eb902a67711a1...|            1|489ae2aa008f02150...|e3b4998c7a498169d...|    2017-02-16 20:37:36|6735.0|       194.31|2017|    2|\n|fefacc66af859508b...|            1|69c590f7ffc7bf8db...|80ceebb4ee9b31afb...|    2018-08-02 04:05:13|6729.0|       193.21|2018|    8|\n|f5136e38d1a14a4db...|            1|1bdf5e6731585cf01...|ee27a8f15b1dded4d...|    2017-06-15 02:45:17|6499.0|       227.66|2017|    6|\n|a96610ab360d42a2e...|            1|a6492cc69376c469a...|59417c56835dd8e2e...|    2017-04-18 13:25:18|4799.0|       151.34|2017|    4|\n|199af31afc78c699f...|            1|c3ed642d592594bb6...|59417c56835dd8e2e...|    2017-05-09 15:50:15|4690.0|        74.34|2017|    5|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 58:===========================================>              (3 + 1) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|c5bdd8ef3c0ec4202...|            2|8a3254bee785a526d...|96804ea39d96eb908...|    2018-05-07 02:55:22| 0.85|         22.3|2018|    5|\n|6e864b3f0ec710311...|            1|8a3254bee785a526d...|96804ea39d96eb908...|    2018-05-02 20:30:34| 0.85|        18.23|2018|    5|\n|3ee6513ae7ea23bdf...|            1|8a3254bee785a526d...|96804ea39d96eb908...|    2018-05-04 03:55:26| 0.85|        18.23|2018|    5|\n|8272b63d03f5f79c5...|            3|05b515fdc76e888aa...|2709af9587499e95e...|    2017-07-21 18:25:23|  1.2|         7.89|2017|    7|\n|8272b63d03f5f79c5...|           13|270516a3f41dc035a...|2709af9587499e95e...|    2017-07-21 18:25:23|  1.2|         7.89|2017|    7|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# arrange data using order by\n\ndf5.orderBy(col(\"price\").desc()).show(5)\n\ndf5.orderBy(col(\"price\").asc(), col(\"freight_value\").desc()).show(5)"}, {"cell_type": "markdown", "id": "6183a3c0", "metadata": {}, "source": "#### Group By Operations"}, {"cell_type": "code", "execution_count": 13, "id": "e00b5dd7", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+----+-----------+---------+-----------------+---------+------------------+\n|year|total_count|min_price|        avg_price|max_price|       total_price|\n+----+-----------+---------+-----------------+---------+------------------+\n|2018|      62511|     0.85|120.0851568523971|   6729.0| 7506643.240000196|\n|2017|      49765|      1.2|121.2673280417994|   6735.0| 6034868.580000147|\n|2016|        370|      6.0|134.5565405405405|   1399.0|49785.919999999984|\n|2020|          4|    69.99|            86.49|    99.99|            345.96|\n+----+-----------+---------+-----------------+---------+------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 69:==================================>                      (6 + 4) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----+-----+-----------+---------+------------------+---------+------------------+\n|year|month|total_count|min_price|         avg_price|max_price|       total_price|\n+----+-----+-----------+---------+------------------+---------+------------------+\n|2016|    9|          4|    44.99| 48.61750000000001|     59.5|194.47000000000003|\n|2016|   10|        365|      6.0|135.83712328767118|   1399.0| 49580.54999999998|\n|2016|   12|          1|     10.9|              10.9|     10.9|              10.9|\n|2017|    1|        681|      2.9|117.65747430249623|   1999.0| 80124.73999999993|\n|2017|    2|       1866|      3.9| 131.8231564844589|   6735.0| 245982.0100000003|\n+----+-----+-----------+---------+------------------+---------+------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# single column\ndf5.groupBy('year').agg(count(\"*\").alias(\"total_count\"),\n                        min(\"price\").alias(\"min_price\"),\n                        avg(\"price\").alias(\"avg_price\"),\n                        max(\"price\").alias(\"max_price\"),\n                        sum(\"price\").alias(\"total_price\")).show(5)\n\n# multi column\ndf5.groupBy('year', 'month').agg(count(\"*\").alias(\"total_count\"),\n                        min(\"price\").alias(\"min_price\"),\n                        avg(\"price\").alias(\"avg_price\"),\n                        max(\"price\").alias(\"max_price\"),\n                        sum(\"price\").alias(\"total_price\")).orderBy(col(\"year\").asc(), col(\"month\").asc()).show(5)"}, {"cell_type": "markdown", "id": "d0dea8b3", "metadata": {}, "source": "### Accumulators\nAccumulator is like a global variable.\naggregate data across all partitions "}, {"cell_type": "code", "execution_count": 14, "id": "3af21a83", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 75:=============================================>           (8 + 2) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "13591643.69999942\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "accum = spark.sparkContext.accumulator(0)  # the variable is initialized with value 0\n\ndf5.foreach(lambda row: accum.add(row[\"price\"])) # each record price is added to accumulator\n\n#accessed by driver\nprint(accum.value)"}, {"cell_type": "markdown", "id": "240529a4", "metadata": {}, "source": "#### Case When Statement\nusually we are deriving a new column by writting case when statement. \nso, in spark to add a new column we use withColumn which takes column name, logic to derive the column\n"}, {"cell_type": "code", "execution_count": 15, "id": "5e03526e", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 76:==============>                                           (1 + 3) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+--------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|price_category|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+--------------+\n|f82a3c6e1e4ad2996...|            1|4052517cac9e78357...|ce27a3cc3c8cc1ea7...|    2017-07-20 02:55:38| 150.0|        16.81|2017|    7|          High|\n|fce3329932f72d112...|            1|0fa81e7123fd0ebe0...|da8622b14eb17ae28...|    2018-02-01 03:16:57| 109.9|        13.79|2018|    2|          High|\n|febc913d8a07693e8...|            1|50f1880f198925172...|7a67c85e85bb2ce85...|    2017-12-18 19:09:57|139.99|        18.23|2017|   12|          High|\n|f489dacdbad0317b1...|            1|1eb4a9aee05c9cad0...|dbc22125167c298ef...|    2017-11-20 11:50:41|  47.9|        16.11|2017|   11|           low|\n|fd74a95ccfea489d3...|            3|382db2ed3015c04d4...|dc4a0fc896dc34b0d...|    2017-10-19 15:07:12| 95.03|        10.75|2017|   10|        Medium|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+--------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df5.withColumn(\"price_category\", when(col(\"price\")>= 100, \"High\")\n                                .when((col(\"price\")<100) & (col(\"price\")>=50), \"Medium\")\n                                .otherwise(\"low\")).show(5)"}, {"cell_type": "markdown", "id": "68cbafea-0001-40b0-afac-cfe4d0ab0838", "metadata": {}, "source": "#### Window Functions\nHow can we apply window functions in spark?\n1. we need to import the Window class first\n2. we need to define window spec i.e. partition by order by clauses"}, {"cell_type": "code", "execution_count": 22, "id": "773df083-1036-4756-a8d1-1fdb3a4c1b25", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+-------+-------------+----+-----+----------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|  price|freight_value|year|month|price_rank|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-------+-------------+----+-----+----------+\n|fefacc66af859508b...|            1|69c590f7ffc7bf8db...|80ceebb4ee9b31afb...|    2018-08-02 04:05:13| 6729.0|       193.21|2018|    8|         1|\n|8dbc85d1447242f3b...|            1|259037a6a41845e45...|c72de06d72748d1a0...|    2018-06-28 12:36:36| 4590.0|        91.78|2018|    6|         2|\n|426a9742b533fc6fe...|            1|a1beef8f3992dbd4c...|512d298ac2a96d193...|    2018-08-16 14:24:28|4399.87|       113.45|2018|    8|         3|\n|68101694e5c5dc733...|            1|6cdf8fc1d741c7658...|ed4acab38528488b6...|    2018-04-05 08:27:27|4099.99|        75.27|2018|    4|         4|\n|b239ca7cd485940b3...|            1|dd113cb02b2af9c8e...|821fb029fc6e495ca...|    2018-08-02 08:15:14| 4059.0|       104.51|2018|    8|         5|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-------+-------------+----+-----+----------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 102:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+------------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|        runnig_sum|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+------------------+\n|b2d1902261f105c5c...|            1|5ed3835ea6f96c77b...|aba1721a889e04dec...|    2018-01-01 22:08:31| 139.0|         8.23|2018|    1|             139.0|\n|3c8e80909dd1066fd...|            1|4308439e0d80d5fe0...|59fb871bf6f4522a8...|    2018-01-01 22:13:24|179.99|         27.8|2018|    1|            318.99|\n|f2e5bcbd102cd01f1...|            1|2bb3e85f2a403543f...|76d64c4aca3a7baf2...|    2018-01-01 22:27:15| 348.9|       118.06|2018|    1|            667.89|\n|2e7080c8c24e4a977...|            1|3bdc89e963c6651b8...|fffd5413c0700ac82...|    2018-01-01 22:30:19| 262.5|       115.49|2018|    1|            930.39|\n|43ac8d91a0749b176...|            1|8dad29b3b6c42d0b9...|4e922959ae960d389...|    2018-01-01 22:36:29| 129.0|        13.92|2018|    1|1059.3899999999999|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Window functions\nfrom pyspark.sql.window import Window\n\n# dense_rank()\nwindowSpec = Window.partitionBy('year').orderBy(col('price').desc())\n\ndf5.withColumn('price_rank', dense_rank().over(windowSpec)).show(5)\n                                                                 \n# cummulative sum\nwindowSpec1 = Window.partitionBy('year').orderBy(col('shipping_limit_datetime').asc())\n\ndf5.withColumn('runnig_sum', sum('price').over(windowSpec1)).show(5)                                                       "}, {"cell_type": "markdown", "id": "37316a51-7638-4ade-af20-6f960208b2e2", "metadata": {}, "source": "#### Joins\n"}, {"cell_type": "code", "execution_count": 23, "id": "fcc6c255-151b-401d-b0ba-95f3b33a1ee2", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- seller_id: string (nullable = true)\n |-- seller_zip_code_prefix: integer (nullable = true)\n |-- seller_city: string (nullable = true)\n |-- seller_state: string (nullable = true)\n\n+--------------------+----------------------+-----------------+------------+\n|           seller_id|seller_zip_code_prefix|      seller_city|seller_state|\n+--------------------+----------------------+-----------------+------------+\n|3442f8959a84dea7e...|                 13023|         campinas|          SP|\n|d1b65fc7debc3361e...|                 13844|       mogi guacu|          SP|\n|ce3ad9de960102d06...|                 20031|   rio de janeiro|          RJ|\n|c0f3eea2e14555b6f...|                  4195|        sao paulo|          SP|\n|51a04a8a6bdcb23de...|                 12914|braganca paulista|          SP|\n|c240c4061717ac180...|                 20920|   rio de janeiro|          RJ|\n|e49c26c3edfa46d22...|                 55325|           brejao|          PE|\n|1b938a7ec6ac5061a...|                 16304|        penapolis|          SP|\n|768a86e36ad6aae3d...|                  1529|        sao paulo|          SP|\n|ccc4bbb5f32a6ab2b...|                 80310|         curitiba|          PR|\n+--------------------+----------------------+-----------------+------------+\nonly showing top 10 rows\n\n"}], "source": "# Read sellers sample data with infer schema ignore header row\n\nhdfs_path =\"/spark_input_data/sellers_dataset.csv\"\nsdf = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(hdfs_path)\n\nsdf.printSchema()\nsdf.show(10)"}, {"cell_type": "code", "execution_count": 26, "id": "c1f97c8b-4784-4cd7-a142-bdb7cd22a531", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+----------------------+-----------+------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|seller_zip_code_prefix|seller_city|seller_state|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+----------------------+-----------+------------+\n|1e1bb536916a99649...|            2|0288f8dd74b931b4e...|1da3aeb70d7989d1e...|    2017-09-05 12:10:11| 49.99|        21.15|2017|    9|                  4265|  sao paulo|          SP|\n|62a0e822dd605871a...|            1|31dbb0d1815bdc83c...|6da1992f915d77be9...|    2017-06-08 11:50:18|  29.0|        15.79|2017|    6|                  1026|  sao paulo|          SP|\n|025c72e88fbf2358b...|            2|bef21943bc2335188...|e49c26c3edfa46d22...|    2017-03-21 21:24:27|  19.9|         20.8|2017|    3|                 55325|     brejao|          PE|\n|23d16dddab46fd3d0...|            1|cca8e09ba6f2d35e4...|43f8c9950d11ecd03...|    2018-01-31 22:17:51|109.99|        14.52|2018|    1|                  6341|carapicuiba|          SP|\n|71c0d1686c9b55563...|            2|eb6c2ecde53034fc9...|1025f0e2d44d7041d...|    2017-12-01 19:31:45| 32.99|        16.11|2017|   12|                  3204|  sao paulo|          SP|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+----------------------+-----------+------------+\nonly showing top 5 rows\n\n"}], "source": "# inner join -- broadcast join\n\nresult1 = df5.join(broadcast(sdf), df5.seller_id == sdf.seller_id, 'inner').drop(sdf.seller_id)\nresult1.show(5)\n\n# result = df5.join(sdf, df5.seller_id == sdf.seller_id, 'inner').join(pdf, df5.product_id == pdf.product_id, 'inner')\n# result.show(10)"}, {"cell_type": "code", "execution_count": 27, "id": "5ddabcf7-db6a-47c7-b518-42d0b33ad78a", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 113:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+----------------------+-----------+------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|seller_zip_code_prefix|seller_city|seller_state|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+----------------------+-----------+------------+\n|1e1bb536916a99649...|            2|0288f8dd74b931b4e...|1da3aeb70d7989d1e...|    2017-09-05 12:10:11| 49.99|        21.15|2017|    9|                  4265|  sao paulo|          SP|\n|62a0e822dd605871a...|            1|31dbb0d1815bdc83c...|6da1992f915d77be9...|    2017-06-08 11:50:18|  29.0|        15.79|2017|    6|                  1026|  sao paulo|          SP|\n|025c72e88fbf2358b...|            2|bef21943bc2335188...|e49c26c3edfa46d22...|    2017-03-21 21:24:27|  19.9|         20.8|2017|    3|                 55325|     brejao|          PE|\n|23d16dddab46fd3d0...|            1|cca8e09ba6f2d35e4...|43f8c9950d11ecd03...|    2018-01-31 22:17:51|109.99|        14.52|2018|    1|                  6341|carapicuiba|          SP|\n|71c0d1686c9b55563...|            2|eb6c2ecde53034fc9...|1025f0e2d44d7041d...|    2017-12-01 19:31:45| 32.99|        16.11|2017|   12|                  3204|  sao paulo|          SP|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+----------------------+-----------+------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# perform join with alias names of dataframes\n\nresult2 = df5.alias('oid').join(sdf.alias('sid'), col('oid.seller_id') == col('sid.seller_id'), 'inner').drop(col('sid.seller_id'))\nresult2.show(5)"}, {"cell_type": "markdown", "id": "288009cd-a519-403d-aa43-342ed83f09e6", "metadata": {}, "source": "## PySpark SQL\nTo work with spark sql, we need to convert data frames in the Temporary View or Temporary Tables.\n1. We will make use of CreateOrReplaceTempView() of dataframe. \n2. write your sql query statements.\n\nThe scope of these temp tables to spark session and dropped when session closes. "}, {"cell_type": "code", "execution_count": 28, "id": "a866548e-ecce-485d-9bc8-e8054c97bfe3", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+--------------------+----------------------+-----------+------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|           seller_id|seller_zip_code_prefix|seller_city|seller_state|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+--------------------+----------------------+-----------+------------+\n|1e1bb536916a99649...|            2|0288f8dd74b931b4e...|1da3aeb70d7989d1e...|    2017-09-05 12:10:11| 49.99|        21.15|2017|    9|1da3aeb70d7989d1e...|                  4265|  sao paulo|          SP|\n|62a0e822dd605871a...|            1|31dbb0d1815bdc83c...|6da1992f915d77be9...|    2017-06-08 11:50:18|  29.0|        15.79|2017|    6|6da1992f915d77be9...|                  1026|  sao paulo|          SP|\n|025c72e88fbf2358b...|            2|bef21943bc2335188...|e49c26c3edfa46d22...|    2017-03-21 21:24:27|  19.9|         20.8|2017|    3|e49c26c3edfa46d22...|                 55325|     brejao|          PE|\n|23d16dddab46fd3d0...|            1|cca8e09ba6f2d35e4...|43f8c9950d11ecd03...|    2018-01-31 22:17:51|109.99|        14.52|2018|    1|43f8c9950d11ecd03...|                  6341|carapicuiba|          SP|\n|71c0d1686c9b55563...|            2|eb6c2ecde53034fc9...|1025f0e2d44d7041d...|    2017-12-01 19:31:45| 32.99|        16.11|2017|   12|1025f0e2d44d7041d...|                  3204|  sao paulo|          SP|\n|029f7a5aabaadfcdf...|            2|0a4093a4af429dc0a...|da8622b14eb17ae28...|    2018-03-09 16:29:19|  29.9|         8.27|2018|    3|da8622b14eb17ae28...|                 13405| piracicaba|          SP|\n|2ee77ee9cd8288db1...|            1|17a019676883dce32...|9f505651f4a6abe90...|    2017-10-26 13:28:07| 23.99|         7.78|2017|   10|9f505651f4a6abe90...|                  4102|  sao paulo|          SP|\n|03053b1cb20006c7f...|            1|3918bb60ff4866202...|d57e18d5f73c7ccb7...|    2018-02-23 13:30:46|  37.9|        16.11|2018|    2|d57e18d5f73c7ccb7...|                  3613|  sao paulo|          SP|\n|097941e2db84b005d...|            1|48f866f20b5b6015a...|4a3ca9315b744ce9f...|    2017-12-01 11:18:26| 153.0|        18.32|2017|   12|4a3ca9315b744ce9f...|                 14940|   ibitinga|          SP|\n|024d4ef645f204e4f...|            2|5b226d7c52f86ac72...|1835b56ce799e6a4d...|    2017-11-07 04:10:21| 56.99|        16.16|2017|   11|1835b56ce799e6a4d...|                 14940|   ibitinga|          SP|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+--------------------+----------------------+-----------+------------+\nonly showing top 10 rows\n\n"}], "source": "# work with Spark SQL\n\ndf5.createOrReplaceTempView('ORDER_ITEMS') # this will convert the dataframe into temp table with the name order_items\nsdf.createOrReplaceTempView('SELLERS')\n\njdf = spark.sql(\" select * from ORDER_ITEMS oid join SELLERS sid on oid.seller_id == sid.seller_id\")\njdf.show(10)"}, {"cell_type": "markdown", "id": "3bab287d-d71b-4b25-bc83-ddba00b9b126", "metadata": {}, "source": "### Write Operations \n\nup until now we did read operations on spark application.\n"}, {"cell_type": "code", "execution_count": 29, "id": "80ab3bf9-107f-4b7d-98b9-0f4396d2bd76", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 121:==================================================>     (9 + 1) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "write successfully\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# write data in HDFS without any partition key\n\nresult1.write.format('csv').option('header', 'true').option('delimiter', ',').save('/tmp/output_data/result1/')\nprint(\"write successfully\")\n\n## Note: when you check the data in hdfs location, you can find the data is stored in 10 partitions. Because we did repartition to 10 earlier on df3 data frame that's why data is stored in 10 partitions\n# Q: whether a new stage is created for this write operation in spark web ui?\n# A: No, because here just 10 parallel tasks for wrote the data and there is no data shuffling or data did not move from one partition to another."}, {"cell_type": "markdown", "id": "77a750b9-67ca-4a0e-b7a1-8db63928ed10", "metadata": {}, "source": "If you have Hive dynamic partitioned external table which reads data from the below mentioned location \nthen data will be automatically fetched by hive server for analysis without reloading the data into tables for each update."}, {"cell_type": "code", "execution_count": null, "id": "d227338a-2f61-4b9b-aa4e-2c93e4edf38a", "metadata": {}, "outputs": [], "source": "# write data in HDFS with partition key\n\nresult1.write.partitionBy('year').format('csv').option('header', 'true').option('delimiter', ',').save('/tmp/output_data/result2/')\nprint(\"write successfully')\n      \n# Q: whether a new stage is created for this write operation in spark web ui?\n# A: Yes a new stage will be created because data shuffling happens to look for specific years data to load into specific partition. "}, {"cell_type": "code", "execution_count": 30, "id": "2d7bc195-2dc7-4003-a74e-73fc8f40c0f7", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 125:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "write successfully\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# write data in HDFS into a single file\n\nresult1.coalesce(1).write.format('csv').option('header', 'true').option('delimiter', ',').save('/tmp/output_data/result3/')\nprint(\"write successfully\")"}, {"cell_type": "markdown", "id": "4c87c2f7-c301-45c7-b8f9-13dec5dbf077", "metadata": {}, "source": "#### Incremental ingestion pipeline \nHere we are getting data from source location and then reading into spark application for doing some transfermations. \nload the transformed data into hive partitioned table."}, {"cell_type": "code", "execution_count": null, "id": "f9dc4489-1e03-4a73-ac22-ee5bec76feb9", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/07/21 17:18:14 WARN SetCommand: 'SET hive.exec.dynamic.partition.mode=nonstrict' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.dynamic.partition.mode) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n24/07/21 17:18:14 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n[Stage 129:============================>                           (5 + 1) / 10]\r"}], "source": "# write data in HIVE directly\nspark.sql(\"\"\"SET hive.exec.dynamic.partition.mode=nonstrict\"\"\")\n\nspark.sql(\"\"\"USE spark_db\"\"\")\n\n# create a partitioned hive table\nspark.sql(\"\"\"CREATE TABLE IF NOT EXISTS orders_sellers_data (\n            order_id STRING,\n            order_item_id INT,\n            product_id STRING,\n            price DOUBLE,\n            freight_value DOUBLE,\n            seller_city STRING\n            )\n            PARTITIONED BY (year INT)\n        \"\"\")\n\n# write data frame to hive table\nresult1.select('order_id',\n               'order_item_id',\n               'product_id',\n               'price',\n               'freight_value',\n               'seller_city',\n               'year').write.mode('append').insertInto('orders_sellers_data')\n\nprint(\"write successfully\")"}, {"cell_type": "code", "execution_count": null, "id": "440792af-c043-49fd-ba5b-aa10873c2e22", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}